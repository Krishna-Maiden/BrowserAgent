{
  "LlmProvider": "ollama", // or "openai"
  "Ollama": {
    "Model": "llama3",
    "Host": "http://localhost:11434"
  },
  "OpenAI": {
    "ApiKey": "sk-proj-xxx"
  },
  "LlmConfig": {
    "UseLocal": true,
  }
}